"""
é•·æ–‡æœ¬ç¸½çµ Agent
æ”¯æŒå°‡é•·æ–‡æœ¬åˆ†å¡Šï¼Œå°æ¯å€‹å¡Šé€²è¡Œç¸½çµï¼Œæœ€å¾Œç”Ÿæˆæ•´é«”ç¸½çµ
æ”¯æŒç•°æ­¥ä¸¦ç™¼è™•ç†å’Œæ›´å¤§çš„æ–‡æœ¬å¡Š
"""
import os
import sys
from typing import List, Optional
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# è™•ç†å°å…¥è·¯å¾‘
try:
    from .chat_completion import chat_completion_simple
except ImportError:
    # å¦‚æœç›¸å°å°å…¥å¤±æ•—ï¼Œå˜—è©¦çµ•å°å°å…¥
    sys.path.insert(0, str(Path(__file__).parent))
    from chat_completion import chat_completion_simple


def split_text_into_chunks(
    text: str,
    chunk_size: int = 100000,  # GPT-4o æ”¯æŒ 128k tokensï¼Œçº¦ç­‰äº 100k-150k å­—ç¬¦ï¼ˆä¸­æ–‡/è‹±æ–‡æ··åˆï¼‰
    chunk_overlap: int = 300  # ç›¸åº”å¢å¤§é‡å éƒ¨åˆ†ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
) -> List[str]:
    """
    å°‡é•·æ–‡æœ¬åˆ†å¡Š
    
    åƒæ•¸:
        text (str): è¦åˆ†å¡Šçš„æ–‡æœ¬
        chunk_size (int): æ¯å¡Šçš„æœ€å¤§å­—ç¬¦æ•¸ï¼Œé»˜èªç‚º 100000ï¼ˆå……åˆ†åˆ©ç”¨ GPT-4o çš„ 128k tokens ä¸Šä¸‹æ–‡ï¼‰
        chunk_overlap (int): å¡Šä¹‹é–“çš„é‡ç–Šå­—ç¬¦æ•¸ï¼Œé»˜èªç‚º 5000
    
    è¿”å›:
        List[str]: æ–‡æœ¬å¡Šåˆ—è¡¨
    
    ç¤ºä¾‹:
        >>> text = "å¾ˆé•·çš„æ–‡æœ¬..."
        >>> chunks = split_text_into_chunks(text, chunk_size=1000)
        >>> print(f"åˆ†æˆ {len(chunks)} å¡Š")
    """
    if not text:
        return []
    
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        # è¨ˆç®—ç•¶å‰å¡Šçš„çµæŸä½ç½®
        end = min(start + chunk_size, text_length)
        
        # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€å¡Šï¼Œå˜—è©¦åœ¨å¥è™Ÿã€æ›è¡Œç¬¦ç­‰ä½ç½®åˆ‡æ–·
        if end < text_length:
            # å°‹æ‰¾åˆé©çš„åˆ†å‰²é»ï¼ˆå„ªå…ˆé¸æ“‡å¥è™Ÿã€å•è™Ÿã€æ„Ÿå˜†è™Ÿã€æ›è¡Œç¬¦ï¼‰
            for separator in ['ã€‚\n', 'ã€‚ ', '\n\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', '\n']:
                last_sep = text.rfind(separator, start, end)
                if last_sep != -1:
                    end = last_sep + len(separator)
                    break
        
        # æå–ç•¶å‰å¡Š
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        # è¨ˆç®—ä¸‹ä¸€å€‹å¡Šçš„èµ·å§‹ä½ç½®ï¼ˆè€ƒæ…®é‡ç–Šï¼‰
        start = max(end - chunk_overlap, start + 1)
    
    return chunks


def summarize_chunk(
    chunk: str,
    chunk_index: int,
    total_chunks: int,
    api_key: str,
    model: str = "chatgpt-4o-latest",
    language: str = "ä¸­æ–‡"
) -> str:
    """
    ç¸½çµå–®å€‹æ–‡æœ¬å¡Š
    
    åƒæ•¸:
        chunk (str): è¦ç¸½çµçš„æ–‡æœ¬å¡Š
        chunk_index (int): ç•¶å‰å¡Šçš„ç´¢å¼•ï¼ˆå¾ 1 é–‹å§‹ï¼‰
        total_chunks (int): ç¸½å¡Šæ•¸
        api_key (str): API å¯†é‘°
        model (str): æ¨¡å‹åç¨±
        language (str): ç¸½çµä½¿ç”¨çš„èªè¨€ï¼Œé»˜èªç‚º "ä¸­æ–‡"
    
    è¿”å›:
        str: è©²å¡Šçš„ç¸½çµ
    """
    system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬ç¸½çµåŠ©æ‰‹ã€‚ä½ çš„ä»»å‹™æ˜¯å°çµ¦å®šçš„æ–‡æœ¬é€²è¡Œç°¡æ½”ã€æº–ç¢ºçš„ç¸½çµã€‚
è¦æ±‚ï¼š
1. æå–æ–‡æœ¬çš„æ ¸å¿ƒè¦é»å’Œé—œéµä¿¡æ¯
2. ä¿æŒé‚è¼¯æ¸…æ™°ï¼Œçµæ§‹å®Œæ•´
3. ä½¿ç”¨{language}é€²è¡Œç¸½çµ
4. ç¸½çµé•·åº¦æ‡‰è©²é©ä¸­ï¼Œæ—¢ä¸èƒ½éºæ¼é‡è¦ä¿¡æ¯ï¼Œä¹Ÿä¸èƒ½éæ–¼å†—é•·
5. å¦‚æœæ–‡æœ¬æ¶‰åŠç‰¹å®šé ˜åŸŸï¼ˆå¦‚æŠ€è¡“ã€ç§‘å­¸ã€æ–‡å­¸ç­‰ï¼‰ï¼Œè«‹ä¿æŒå°ˆæ¥­æ€§"""
    
    prompt = f"""è«‹ç¸½çµä»¥ä¸‹æ–‡æœ¬ï¼ˆç¬¬ {chunk_index}/{total_chunks} å¡Šï¼‰ï¼š

{chunk}

è«‹æä¾›ä¸€å€‹æ¸…æ™°ã€ç°¡æ½”çš„ç¸½çµï¼Œçªå‡ºæ ¸å¿ƒè¦é»å’Œé—œéµä¿¡æ¯ã€‚"""
    
    try:
        summary = chat_completion_simple(
            prompt=prompt,
            api_key=api_key,
            model=model,
            system_prompt=system_prompt,
            temperature=0.3,  # è¼ƒä½çš„æº«åº¦ä»¥ä¿è­‰ç¸½çµçš„ä¸€è‡´æ€§å’Œæº–ç¢ºæ€§
            max_tokens=8000  # å¢å¤§è¾“å‡º token é™åˆ¶ï¼Œå……åˆ†åˆ©ç”¨ GPT-4o çš„èƒ½åŠ›
        )
        return summary
    except Exception as e:
        print(f"âš ï¸ ç¸½çµç¬¬ {chunk_index} å¡Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return f"[ç¸½çµå¤±æ•—: {str(e)}]"


def summarize_text(
    text: str,
    api_key: Optional[str] = None,
    model: str = "chatgpt-4o-latest",
    chunk_size: int = 100000,  # GPT-4o æ”¯æŒ 128k tokensï¼Œçº¦ç­‰äº 100k-150k å­—ç¬¦
    chunk_overlap: int = 300,  # ç›¸åº”å¢å¤§é‡å ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
    language: str = "ä¸­æ–‡",
    show_progress: bool = True,
    enable_async: bool = True,
    max_workers: int = 5  # å¹¶å‘æ€»ç»“çš„çº¿ç¨‹æ•°
) -> str:
    """
    ç¸½çµé•·æ–‡æœ¬çš„ä¸»å‡½æ•¸
    
    åƒæ•¸:
        text (str): è¦ç¸½çµçš„é•·æ–‡æœ¬
        api_key (str, optional): API å¯†é‘°ï¼Œå¦‚æœç‚º None å‰‡å¾ç’°å¢ƒè®Šé‡è®€å–
        model (str): æ¨¡å‹åç¨±ï¼Œé»˜èªç‚º "chatgpt-4o-latest"
        chunk_size (int): æ¯å¡Šçš„æœ€å¤§å­—ç¬¦æ•¸ï¼Œé»˜èªç‚º 100000ï¼ˆå……åˆ†åˆ©ç”¨ GPT-4o çš„ 128k tokens ä¸Šä¸‹æ–‡ï¼‰
        chunk_overlap (int): å¡Šä¹‹é–“çš„é‡ç–Šå­—ç¬¦æ•¸ï¼Œé»˜èªç‚º 5000
        language (str): ç¸½çµä½¿ç”¨çš„èªè¨€ï¼Œé»˜èªç‚º "ä¸­æ–‡"
        show_progress (bool): æ˜¯å¦é¡¯ç¤ºé€²åº¦ï¼Œé»˜èªç‚º True
        enable_async (bool): æ˜¯å¦å•Ÿç”¨ç•°æ­¥ä¸¦ç™¼ç¸½çµï¼Œé»˜èªç‚º True
        max_workers (int): ä¸¦ç™¼ç¸½çµçš„æœ€å¤§ç·šç¨‹æ•¸ï¼Œé»˜èªç‚º 5
    
    è¿”å›:
        str: æœ€çµ‚çš„æ–‡æœ¬ç¸½çµ
    
    ç¤ºä¾‹:
        >>> long_text = "å¾ˆé•·çš„æ–‡æœ¬å…§å®¹..."
        >>> summary = summarize_text(long_text, api_key="your-api-key")
        >>> print(summary)
    """
    # ç²å– API key
    if api_key is None:
        api_key = os.getenv("API_KEY_302_AI") or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("è«‹æä¾› API å¯†é‘°æˆ–è¨­ç½®ç’°å¢ƒè®Šé‡ API_KEY_302_AI æˆ– OPENAI_API_KEY")
    
    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬ä¸èƒ½ç‚ºç©º")
    
    # æ­¥é©Ÿ 1: å°‡æ–‡æœ¬åˆ†å¡Š
    if show_progress:
        print(f"ğŸ“ æ­£åœ¨å°‡æ–‡æœ¬åˆ†å¡Šï¼ˆå¡Šå¤§å°: {chunk_size}, é‡ç–Š: {chunk_overlap}ï¼‰...")
    
    chunks = split_text_into_chunks(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    
    if not chunks:
        raise ValueError("æ–‡æœ¬åˆ†å¡Šå¤±æ•—ï¼Œæœªç”Ÿæˆä»»ä½•å¡Š")
    
    total_chunks = len(chunks)
    if show_progress:
        print(f"âœ“ æ–‡æœ¬å·²åˆ†æˆ {total_chunks} å¡Š\n")
    
    # å¦‚æœåªæœ‰ä¸€å¡Šï¼Œç›´æ¥ç¸½çµ
    if total_chunks == 1:
        if show_progress:
            print("ğŸ“Š æ–‡æœ¬è¼ƒçŸ­ï¼Œç›´æ¥é€²è¡Œç¸½çµ...")
        return summarize_chunk(
            chunks[0],
            chunk_index=1,
            total_chunks=1,
            api_key=api_key,
            model=model,
            language=language
        )
    
    # æ­¥é©Ÿ 2: å°æ¯å€‹å¡Šé€²è¡Œç¸½çµï¼ˆæ”¯æŒä¸¦ç™¼ï¼‰
    if show_progress:
        if enable_async:
            print(f"ğŸ“‹ é–‹å§‹ä¸¦ç™¼ç¸½çµå„å€‹æ–‡æœ¬å¡Šï¼ˆæœ€å¤§ {max_workers} å€‹ç·šç¨‹ï¼‰...\n")
        else:
            print(f"ğŸ“‹ é–‹å§‹ç¸½çµå„å€‹æ–‡æœ¬å¡Š...\n")
    
    chunk_summaries = []
    
    if enable_async and total_chunks > 1:
        # ä½¿ç”¨ç·šç¨‹æ± ä¸¦ç™¼ç¸½çµ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_chunk = {}
            
            for i, chunk in enumerate(chunks, start=1):
                future = executor.submit(
                    summarize_chunk,
                    chunk,
                    chunk_index=i,
                    total_chunks=total_chunks,
                    api_key=api_key,
                    model=model,
                    language=language
                )
                future_to_chunk[future] = i
            
            # æ”¶é›†çµæœï¼ˆæŒ‰é †åºï¼‰
            completed = 0
            results_dict = {}  # ä½¿ç”¨å­—å…¸ä¿å­˜çµæœï¼Œä»¥ä¿æŒé †åº
            
            for future in as_completed(future_to_chunk):
                chunk_idx = future_to_chunk[future]
                try:
                    summary = future.result()
                    results_dict[chunk_idx] = summary
                    completed += 1
                    
                    if show_progress:
                        print(f"  âœ“ å®Œæˆç¬¬ {chunk_idx}/{total_chunks} å¡Š ({completed}/{total_chunks})")
                except Exception as e:
                    print(f"  âš ï¸ ç¸½çµç¬¬ {chunk_idx} å¡Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    results_dict[chunk_idx] = f"[ç¸½çµå¤±æ•—: {str(e)}]"
            
            # æŒ‰é †åºçµ„è£çµæœ
            chunk_summaries = [results_dict[i] for i in range(1, total_chunks + 1) if i in results_dict]
    else:
        # é †åºè™•ç†
        for i, chunk in enumerate(chunks, start=1):
            if show_progress:
                print(f"  è™•ç†ç¬¬ {i}/{total_chunks} å¡Š...", end=" ", flush=True)
            
            summary = summarize_chunk(
                chunk,
                chunk_index=i,
                total_chunks=total_chunks,
                api_key=api_key,
                model=model,
                language=language
            )
            
            chunk_summaries.append(summary)
            
            if show_progress:
                print("âœ“")
    
    # æ­¥é©Ÿ 3: åˆä½µæ‰€æœ‰å¡Šçš„ç¸½çµï¼Œç”Ÿæˆæœ€çµ‚ç¸½çµ
    if show_progress:
        print(f"\nğŸ“‘ æ­£åœ¨ç”Ÿæˆæœ€çµ‚ç¸½çµ...")
    
    # åˆä½µæ‰€æœ‰å¡Šçš„ç¸½çµ
    combined_summaries = "\n\n".join([
        f"ç¬¬ {i+1} å¡Šç¸½çµï¼š\n{summary}"
        for i, summary in enumerate(chunk_summaries)
    ])
    
    system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬ç¸½çµåŠ©æ‰‹ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šå¤šå€‹æ–‡æœ¬å¡Šçš„ç¸½çµï¼Œç”Ÿæˆä¸€å€‹å®Œæ•´ã€é€£è²«çš„æ•´é«”ç¸½çµã€‚
è¦æ±‚ï¼š
1. æ•´åˆæ‰€æœ‰å¡Šçš„ç¸½çµï¼Œå½¢æˆä¸€å€‹é‚è¼¯æ¸…æ™°çš„æ•´é«”ç¸½çµ
2. æ¶ˆé™¤é‡è¤‡ä¿¡æ¯ï¼Œçªå‡ºæ ¸å¿ƒè¦é»
3. ä¿æŒç¸½çµçš„å®Œæ•´æ€§å’Œé€£è²«æ€§
4. ä½¿ç”¨{language}é€²è¡Œç¸½çµ
5. ç¢ºä¿ç¸½çµèƒ½å¤ å…¨é¢åæ˜ åŸæ–‡çš„æ ¸å¿ƒå…§å®¹å’Œä¸»è¦è§€é»"""
    
    final_prompt = f"""ä»¥ä¸‹æ˜¯å°é•·æ–‡æœ¬å„å€‹éƒ¨åˆ†çš„ç¸½çµï¼š

{combined_summaries}

è«‹æ ¹æ“šä»¥ä¸Šå„å€‹éƒ¨åˆ†çš„ç¸½çµï¼Œç”Ÿæˆä¸€å€‹å®Œæ•´ã€é€£è²«çš„æ•´é«”ç¸½çµã€‚è¦æ±‚ï¼š
1. æ•´åˆæ‰€æœ‰é—œéµä¿¡æ¯ï¼Œå½¢æˆé‚è¼¯æ¸…æ™°çš„ç¸½çµ
2. æ¶ˆé™¤é‡è¤‡å…§å®¹
3. çªå‡ºæ ¸å¿ƒè§€é»å’Œä¸»è¦è«–è¿°
4. ä¿æŒçµæ§‹å®Œæ•´ï¼Œèªè¨€æµæš¢"""
    
    try:
        # å……åˆ†åˆ©ç”¨ GPT-4o çš„ 128k tokens ä¸Šä¸‹æ–‡ï¼Œå¢å¤§ max_tokens è¾“å‡ºé™åˆ¶
        final_summary = chat_completion_simple(
            prompt=final_prompt,
            api_key=api_key,
            model=model,
            system_prompt=system_prompt,
            temperature=0.3,
            max_tokens=16000  # å¢å¤§ä»¥å……åˆ†åˆ©ç”¨ GPT-4o çš„èƒ½åŠ›ç”Ÿæˆæ›´è¯¦ç»†çš„æ€»ç»“
        )
        
        if show_progress:
            print("âœ“ ç¸½çµå®Œæˆï¼\n")
        
        return final_summary
    except Exception as e:
        raise Exception(f"ç”Ÿæˆæœ€çµ‚ç¸½çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")


if __name__ == "__main__":
    import sys
    
    print("=" * 60)
    print("é•·æ–‡æœ¬ç¸½çµ Agent")
    print("=" * 60)
    
    # å¾ç’°å¢ƒè®Šé‡ç²å– API key
    api_key = os.getenv("API_KEY_302_AI") or os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        print("\nâŒ éŒ¯èª¤: è«‹è¨­ç½®ç’°å¢ƒè®Šé‡ API_KEY_302_AI æˆ– OPENAI_API_KEY")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  export API_KEY_302_AI='your-api-key'")
        print("  python summarize_text.py <æ–‡æœ¬æ–‡ä»¶è·¯å¾‘>")
        sys.exit(1)
    
    if len(sys.argv) < 2:
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python summarize_text.py <æ–‡æœ¬æ–‡ä»¶è·¯å¾‘> [å¡Šå¤§å°] [æ¨¡å‹åç¨±]")
        print("\nç¤ºä¾‹:")
        print("  python summarize_text.py document.txt")
        print("  python summarize_text.py document.txt 2000 chatgpt-4o-latest")
        sys.exit(1)
    
    file_path = sys.argv[1]
    chunk_size = int(sys.argv[2]) if len(sys.argv) > 2 else 2000
    model = sys.argv[3] if len(sys.argv) > 3 else "chatgpt-4o-latest"
    
    try:
        # è®€å–æ–‡æœ¬æ–‡ä»¶
        print(f"\nğŸ“– è®€å–æ–‡ä»¶: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        if not text.strip():
            print("âŒ éŒ¯èª¤: æ–‡ä»¶ç‚ºç©º")
            sys.exit(1)
        
        print(f"âœ“ æ–‡ä»¶é•·åº¦: {len(text)} å­—ç¬¦\n")
        
        # åŸ·è¡Œç¸½çµ
        summary = summarize_text(
            text=text,
            api_key=api_key,
            model=model,
            chunk_size=chunk_size,
            show_progress=True
        )
        
        print("=" * 60)
        print("æœ€çµ‚ç¸½çµ:")
        print("=" * 60)
        print(summary)
        print("\n" + "=" * 60)
        
        # å¯é¸ï¼šä¿å­˜ç¸½çµåˆ°æ–‡ä»¶
        output_file = file_path.rsplit('.', 1)[0] + "_summary.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("é•·æ–‡æœ¬ç¸½çµ\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"åŸæ–‡æ–‡ä»¶: {file_path}\n")
            f.write(f"åŸæ–‡é•·åº¦: {len(text)} å­—ç¬¦\n\n")
            f.write("=" * 60 + "\n")
            f.write("ç¸½çµ:\n")
            f.write("=" * 60 + "\n\n")
            f.write(summary)
        
        print(f"\nğŸ’¾ ç¸½çµå·²ä¿å­˜åˆ°: {output_file}")
        
    except FileNotFoundError:
        print(f"\nâŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æ–‡ä»¶ '{file_path}'")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

