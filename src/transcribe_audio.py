"""
ä½¿ç”¨WhisperXå°‡éŸ³é »è½‰æ›ç‚ºæ–‡æœ¬
æ”¯æŒè©ç´šåˆ¥æ™‚é–“æˆ³å’Œèªªè©±äººåˆ†é›¢ï¼ˆdiarizationï¼‰
æ”¯æŒåˆ†å¡Šè½‰éŒ„åŠ é€Ÿå’Œä¸¦ç™¼è™•ç†
"""
import warnings
# æŠ‘åˆ¶ torchaudio å¼ƒç”¨è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='torchaudio')
warnings.filterwarnings('ignore', category=UserWarning, module='pyannote')

import whisperx
import gc
import os
import subprocess
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple, Callable
from concurrent.futures import ThreadPoolExecutor, as_completed

# æŠ‘åˆ¶ WhisperX å¯¹é½è­¦å‘Š
logging.getLogger("whisperx.alignment").setLevel(logging.ERROR)


def get_audio_duration(audio_file: str) -> float:
    """
    è·å–éŸ³é¢‘æ–‡ä»¶çš„æ—¶é•¿ï¼ˆç§’ï¼‰
    
    å‚æ•°:
        audio_file (str): éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        float: éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
    """
    try:
        # ä½¿ç”¨ ffprobe è·å–éŸ³é¢‘æ—¶é•¿
        cmd = [
            'ffprobe', '-v', 'error', '-show_entries',
            'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1',
            audio_file
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        duration = float(result.stdout.strip())
        return duration
    except (subprocess.CalledProcessError, ValueError, FileNotFoundError) as e:
        # å¦‚æœ ffprobe ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨ whisperx åŠ è½½éŸ³é¢‘æ¥ä¼°ç®—
        try:
            audio = whisperx.load_audio(audio_file)
            # å‡è®¾é‡‡æ ·ç‡ä¸º 16000 (whisperx é»˜è®¤)
            duration = len(audio) / 16000.0
            return duration
        except Exception:
            print(f"âš  æ— æ³•è·å–éŸ³é¢‘æ—¶é•¿: {e}")
            return 0.0


def estimate_transcription_time(
    audio_duration: float,
    model_name: str = "base",
    device: str = "cpu"
) -> float:
    """
    ä¼°ç®—è½¬å½•æ‰€éœ€æ—¶é—´ï¼ˆç§’ï¼‰
    
    å‚æ•°:
        audio_duration (float): éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        model_name (str): æ¨¡å‹åç§°
        device (str): è®¾å¤‡ç±»å‹
    
    è¿”å›:
        float: ä¼°ç®—çš„è½¬å½•æ—¶é—´ï¼ˆç§’ï¼‰
    """
    # ä¸åŒæ¨¡å‹çš„ç›¸å¯¹é€Ÿåº¦ï¼ˆç›¸å¯¹äº base æ¨¡å‹ï¼‰
    model_speeds = {
        "tiny": 0.3,    # éå¸¸å¿«
        "base": 1.0,    # åŸºå‡†
        "small": 2.0,   # è¾ƒæ…¢
        "medium": 4.0,  # æ…¢
        "large-v2": 6.0,
        "large-v3": 6.0
    }
    
    # è®¾å¤‡å› å­
    device_factors = {
        "cuda": 0.5,    # GPU å¿« 2 å€
        "cpu": 1.0,
        "mps": 0.8      # MPS ç¨å¿«
    }
    
    base_speed = model_speeds.get(model_name, 1.0)
    device_factor = device_factors.get(device, 1.0)
    
    # è½¬å½•æ—¶é—´é€šå¸¸æ˜¯éŸ³é¢‘æ—¶é•¿çš„ base_speed / device_factor å€
    # åŠ ä¸Šå›ºå®šçš„æ¨¡å‹åŠ è½½å’Œå¯¹é½æ—¶é—´ï¼ˆçº¦ 10-30 ç§’ï¼‰
    overhead = 20.0  # æ¨¡å‹åŠ è½½å’Œå¯¹é½çš„å›ºå®šå¼€é”€
    
    transcription_time = (audio_duration * base_speed / device_factor) + overhead
    
    # å¦‚æœæœ‰åˆ†å—å¤„ç†ï¼Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ˆå¹¶å‘ä¼˜åŠ¿ï¼‰
    return transcription_time


def split_audio_file(
    audio_file: str,
    chunk_duration: float = 60.0,
    output_dir: Optional[str] = None
) -> List[Tuple[str, float, float]]:
    """
    å°†éŸ³é¢‘æ–‡ä»¶æŒ‰æ—¶é—´åˆ†å‰²æˆå¤šä¸ªå—
    
    å‚æ•°:
        audio_file (str): è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        chunk_duration (float): æ¯å—çš„æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 60 ç§’
        output_dir (str, optional): è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºä¸´æ—¶ç›®å½•
    
    è¿”å›:
        List[Tuple[str, float, float]]: [(å—æ–‡ä»¶è·¯å¾„, å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´), ...]
    """
    duration = get_audio_duration(audio_file)
    
    if duration == 0:
        # å¦‚æœæ— æ³•è·å–æ—¶é•¿ï¼Œè¿”å›åŸæ–‡ä»¶
        return [(audio_file, 0.0, duration)]
    
    if duration <= chunk_duration:
        # éŸ³é¢‘å¤ªçŸ­ï¼Œä¸éœ€è¦åˆ†å‰²
        return [(audio_file, 0.0, duration)]
    
    if output_dir is None:
        output_dir = os.path.dirname(os.path.abspath(audio_file))
    
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    chunks = []
    base_name = Path(audio_file).stem
    chunk_index = 0
    
    start_time = 0.0
    
    while start_time < duration:
        end_time = min(start_time + chunk_duration, duration)
        
        # ä½¿ç”¨ ffmpeg åˆ‡å‰²éŸ³é¢‘
        chunk_file = os.path.join(output_dir, f"{base_name}_chunk_{chunk_index:04d}.wav")
        
        try:
            cmd = [
                'ffmpeg', '-i', audio_file,
                '-ss', str(start_time),
                '-t', str(end_time - start_time),
                '-acodec', 'pcm_s16le',
                '-ar', '16000',
                '-ac', '1',
                '-y',  # è¦†ç›–å·²å­˜åœ¨æ–‡ä»¶
                chunk_file
            ]
            subprocess.run(cmd, capture_output=True, check=True)
            chunks.append((chunk_file, start_time, end_time))
        except (subprocess.CalledProcessError, FileNotFoundError) as e:
            print(f"âš  åˆ‡å‰²éŸ³é¢‘å—å¤±è´¥: {e}")
            # å¦‚æœ ffmpeg ä¸å¯ç”¨ï¼Œè¿”å›åŸæ–‡ä»¶
            if chunk_index == 0:
                return [(audio_file, 0.0, duration)]
            break
        
        start_time = end_time
        chunk_index += 1
    
    return chunks


def transcribe_chunk(
    chunk_file: str,
    chunk_start: float,
    model,
    model_name: str,
    device: str,
    batch_size: int,
    language: Optional[str],
    align_model: Optional[Any],
    align_metadata: Optional[Any],
    audio_data: Any
) -> Dict[str, Any]:
    """
    è½¬å½•å•ä¸ªéŸ³é¢‘å—
    
    å‚æ•°:
        chunk_file (str): éŸ³é¢‘å—æ–‡ä»¶è·¯å¾„
        chunk_start (float): å—çš„èµ·å§‹æ—¶é—´ï¼ˆç”¨äºè°ƒæ•´æ—¶é—´æˆ³ï¼‰
        model: WhisperX æ¨¡å‹å¯¹è±¡
        model_name (str): æ¨¡å‹åç§°
        device (str): è®¾å¤‡ç±»å‹
        batch_size (int): æ‰¹æ¬¡å¤§å°
        language (str, optional): è¯­è¨€ä»£ç 
        align_model: å¯¹é½æ¨¡å‹
        align_metadata: å¯¹é½å…ƒæ•°æ®
        audio_data: éŸ³é¢‘æ•°æ®
    
    è¿”å›:
        dict: è½¬å½•ç»“æœ
    """
    try:
        # åŠ è½½éŸ³é¢‘å—
        chunk_audio = whisperx.load_audio(chunk_file)
        
        # è½¬å½•
        result = model.transcribe(chunk_audio, batch_size=batch_size, language=language)
        
        # è°ƒæ•´æ—¶é—´æˆ³ï¼ˆåŠ ä¸Šå—çš„èµ·å§‹æ—¶é—´ï¼‰
        for segment in result['segments']:
            segment['start'] += chunk_start
            segment['end'] += chunk_start
            
            # è°ƒæ•´è¯çº§æ—¶é—´æˆ³
            if 'words' in segment:
                for word in segment['words']:
                    word['start'] += chunk_start
                    word['end'] += chunk_start
        
        # å¯¹é½æ—¶é—´æˆ³ï¼ˆå¦‚æœæä¾›äº†å¯¹é½æ¨¡å‹ï¼‰
        if align_model and align_metadata:
            try:
                # è¿‡æ»¤æ‰ç©ºæ–‡æœ¬æˆ–æ— æ•ˆçš„æ®µè½
                valid_segments = [
                    seg for seg in result["segments"]
                    if seg.get('text', '').strip() and len(seg.get('text', '').strip()) > 0
                ]
                
                if valid_segments:
                    aligned_result = whisperx.align(
                        valid_segments,
                        align_model,
                        align_metadata,
                        chunk_audio,
                        device,
                        return_char_alignments=False
                    )
                    
                    # ä½¿ç”¨å¯¹é½åçš„ç»“æœ
                    result['segments'] = aligned_result.get('segments', valid_segments)
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ®µè½ï¼Œä¿æŒåŸç»“æœ
                    pass
                    
            except Exception as align_error:
                # å¯¹é½å¤±è´¥æ—¶ï¼Œä¿æŒåŸå§‹è½¬å½•ç»“æœï¼ˆä¸å¸¦è¯çº§æ—¶é—´æˆ³ï¼‰
                print(f"âš  å¯¹é½å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è½¬å½•ç»“æœ: {align_error}")
                # ç§»é™¤å¯èƒ½å­˜åœ¨çš„æ— æ•ˆè¯çº§æ—¶é—´æˆ³
                for segment in result['segments']:
                    if 'words' in segment:
                        # åªä¿ç•™æœ‰æ•ˆçš„è¯çº§æ—¶é—´æˆ³
                        segment['words'] = [
                            w for w in segment.get('words', [])
                            if w.get('word', '').strip() and w.get('start', 0) >= 0 and w.get('end', 0) > w.get('start', 0)
                        ]
            
            # è°ƒæ•´å¯¹é½åçš„æ—¶é—´æˆ³
            for segment in result.get('segments', []):
                segment['start'] += chunk_start
                segment['end'] += chunk_start
                if 'words' in segment:
                    for word in segment['words']:
                        word['start'] += chunk_start
                        word['end'] += chunk_start
        
        return result
    except Exception as e:
        print(f"âš  è½¬å½•å—å¤±è´¥ {chunk_file}: {e}")
        return {'segments': [], 'language': language or 'unknown'}


def merge_transcription_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    åˆå¹¶å¤šä¸ªè½¬å½•ç»“æœ
    
    å‚æ•°:
        results (List[Dict[str, Any]]): å¤šä¸ªè½¬å½•ç»“æœåˆ—è¡¨
    
    è¿”å›:
        dict: åˆå¹¶åçš„è½¬å½•ç»“æœ
    """
    if not results:
        return {'segments': [], 'language': 'unknown'}
    
    merged = {
        'segments': [],
        'language': results[0].get('language', 'unknown')
    }
    
    # æŒ‰æ—¶é—´æˆ³æ’åºå¹¶åˆå¹¶æ‰€æœ‰æ®µè½
    all_segments = []
    for result in results:
        all_segments.extend(result.get('segments', []))
    
    # æŒ‰å¼€å§‹æ—¶é—´æ’åº
    all_segments.sort(key=lambda x: x.get('start', 0))
    
    merged['segments'] = all_segments
    return merged


def transcribe_audio(
    audio_file: str,
    model_name: str = "base",
    device: str = "auto",
    compute_type: str = "auto",
    batch_size: int = 16,  # Mç³»åˆ—èŠ¯ç‰‡å¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch_sizeï¼ˆå¦‚32-64ï¼‰ç²å¾—æ›´å¥½æ€§èƒ½
    language: Optional[str] = None,
    diarize: bool = False,
    hf_token: Optional[str] = None,
    output_dir: Optional[str] = None,
    highlight_words: bool = False,
    enable_chunking: bool = True,
    chunk_duration: float = 60.0,
    max_workers: int = 4,
    progress_callback: Optional[Callable[[int, int, str], None]] = None
) -> Dict[str, Any]:
    """
    ä½¿ç”¨WhisperXè½‰éŒ„éŸ³é »æ–‡ä»¶
    
    åƒæ•¸:
        audio_file (str): éŸ³é »æ–‡ä»¶è·¯å¾‘
        model_name (str): Whisperæ¨¡å‹åç¨± (tiny/base/small/medium/large-v2/large-v3)
        device (str): è¨ˆç®—è¨­å‚™ ("cpu", "cuda", "auto") - æ³¨æ„ï¼šWhisperXç›®å‰ä¸æ”¯æŒMPSè¨­å‚™
        compute_type (str): è¨ˆç®—é¡å‹ ("float16", "int8", "auto")
        batch_size (int): æ‰¹æ¬¡å¤§å°ï¼ˆé™ä½ä»¥æ¸›å°‘GPUå…§å­˜ä½¿ç”¨ï¼‰
        language (str, optional): èªè¨€ä»£ç¢¼ï¼ˆå¦‚ "en", "zh", "de"ï¼‰ï¼ŒNoneç‚ºè‡ªå‹•æª¢æ¸¬
        diarize (bool): æ˜¯å¦é€²è¡Œèªªè©±äººåˆ†é›¢
        hf_token (str, optional): HuggingFace tokenï¼ˆç”¨æ–¼èªªè©±äººåˆ†é›¢ï¼‰
        output_dir (str, optional): è¼¸å‡ºç›®éŒ„ï¼Œé»˜èªç‚ºéŸ³é »æ–‡ä»¶æ‰€åœ¨ç›®éŒ„
        highlight_words (bool): æ˜¯å¦åœ¨SRTæ–‡ä»¶ä¸­é«˜äº®è©ç´šæ™‚é–“æˆ³
        enable_chunking (bool): æ˜¯å¦å•Ÿç”¨åˆ†å¡Šè½‰éŒ„åŠ é€Ÿï¼Œé»˜èªç‚º True
        chunk_duration (float): æ¯å¡Šçš„æ™‚é•·ï¼ˆç§’ï¼‰ï¼Œé»˜èªç‚º 60.0
        max_workers (int): ä¸¦ç™¼è½‰éŒ„çš„æœ€å¤§ç·šç¨‹æ•¸ï¼Œé»˜èªç‚º 4
        progress_callback (callable, optional): é€²åº¦å›èª¿å‡½æ•¸ï¼Œæ¥æ”¶ (current, total, message) åƒæ•¸
    
    è¿”å›:
        dict: åŒ…å«è½‰éŒ„çµæœçš„å­—å…¸ï¼ŒåŒ…å« 'duration' å’Œ 'estimated_time' å­—æ®µ
    """
    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(audio_file):
        raise FileNotFoundError(f"éŸ³é »æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
    
    # è‡ªå‹•æª¢æ¸¬è¨­å‚™
    if device == "auto":
        try:
            import torch
            # æ³¨æ„ï¼šWhisperXç›®å‰ä¸æ”¯æŒMPSè¨­å‚™ï¼Œæ‰€ä»¥è·³éMPSæª¢æ¸¬
            # å„ªå…ˆæª¢æ¸¬CUDAï¼ˆNVIDIA GPUï¼‰
            if torch.cuda.is_available():
                device = "cuda"
                print("âœ“ æª¢æ¸¬åˆ°CUDA GPUï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
            # å°æ–¼Apple Siliconï¼Œé›–ç„¶æœ‰MPSï¼Œä½†WhisperXä¸æ”¯æŒï¼Œæ‰€ä»¥ä½¿ç”¨CPU
            # ä½†å¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch_sizeå’Œå„ªåŒ–çš„è¨ˆç®—é¡å‹ä¾†æå‡æ€§èƒ½
            else:
                device = "cpu"
                # æª¢æŸ¥æ˜¯å¦ç‚ºApple Silicon
                if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    print("â„¹ï¸  æª¢æ¸¬åˆ°Apple Siliconï¼Œé›–ç„¶WhisperXä¸æ”¯æŒMPSï¼Œä½†å·²å„ªåŒ–CPUè¨­ç½®")
                    print("ğŸ’¡ æç¤ºï¼šMç³»åˆ—èŠ¯ç‰‡ä½¿ç”¨CPUæ¨¡å¼ï¼Œå»ºè­°ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹ï¼ˆbase/smallï¼‰ç²å¾—æ›´å¥½æ€§èƒ½")
                else:
                    print("âš  æœªæª¢æ¸¬åˆ°GPUï¼Œä½¿ç”¨CPUï¼ˆé€Ÿåº¦è¼ƒæ…¢ï¼‰")
        except ImportError:
            device = "cpu"
            print("âš  PyTorchæœªå®‰è£ï¼Œä½¿ç”¨CPU")
    
    # è‡ªå‹•é¸æ“‡è¨ˆç®—é¡å‹
    if compute_type == "auto":
        if device == "cpu":
            # å°æ–¼Apple Siliconï¼Œå³ä½¿ä½¿ç”¨CPUï¼Œä¹Ÿå¯ä»¥å˜—è©¦ä½¿ç”¨æ›´å¿«çš„è¨­ç½®
            import torch
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                # Apple Siliconçš„CPUæ€§èƒ½å¾ˆå¥½ï¼Œå¯ä»¥ä½¿ç”¨int8ç²å¾—æ›´å¥½æ€§èƒ½
                compute_type = "int8"
            else:
                compute_type = "int8"
        else:
            compute_type = "float16"
    
    # è¨­ç½®è¼¸å‡ºç›®éŒ„
    if output_dir is None:
        output_dir = os.path.dirname(os.path.abspath(audio_file))
    
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # è·å–éŸ³é¢‘æ—¶é•¿
    audio_duration = get_audio_duration(audio_file)
    estimated_time = estimate_transcription_time(audio_duration, model_name, device)
    
    print(f"é–‹å§‹è½‰éŒ„: {audio_file}")
    print(f"éŸ³é »æ™‚é•·: {audio_duration:.1f} ç§’ ({audio_duration/60:.1f} åˆ†é˜)")
    print(f"é è¨ˆè½‰éŒ„æ™‚é–“: {estimated_time:.1f} ç§’ ({estimated_time/60:.1f} åˆ†é˜)")
    print(f"æ¨¡å‹: {model_name}")
    print(f"è¨­å‚™: {device}")
    print(f"è¨ˆç®—é¡å‹: {compute_type}")
    
    try:
        # æª¢æŸ¥æœ¬åœ°ç·©å­˜
        cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
        model_repo = f"Systran/faster-whisper-{model_name}"
        model_cache_path = os.path.join(cache_dir, f"models--{model_repo.replace('/', '--')}")
        
        if os.path.exists(model_cache_path):
            print(f"âœ“ ç™¼ç¾æœ¬åœ°æ¨¡å‹ç·©å­˜ï¼Œä½¿ç”¨ç·©å­˜ä¸­çš„æ¨¡å‹...")
            os.environ.setdefault('HF_HUB_DOWNLOAD_TIMEOUT', '300')
        
        # æ±ºå®šæ˜¯å¦ä½¿ç”¨åˆ†å¡Šè½‰éŒ„
        should_chunk = enable_chunking and audio_duration > chunk_duration * 1.5  # åªæœ‰ç•¶æ™‚é•·è¶…é1.5å€å¡Šå¤§å°æ™‚æ‰åˆ†å¡Š
        
        if should_chunk:
            print(f"\n[ä½¿ç”¨åˆ†å¡Šè½‰éŒ„] éŸ³é »æ™‚é•· {audio_duration:.1f} ç§’ï¼Œå°‡åˆ†å¡Šè™•ç†...")
            
            # 1. åˆ‡å‰²éŸ³é »
            if progress_callback:
                progress_callback(0, 100, "æ­£åœ¨åˆ‡å‰²éŸ³é »æ–‡ä»¶...")
            chunks = split_audio_file(audio_file, chunk_duration, output_dir)
            print(f"âœ“ å·²åˆ‡å‰²æˆ {len(chunks)} å€‹å¡Š")
            
            if progress_callback:
                progress_callback(10, 100, f"å·²åˆ‡å‰²æˆ {len(chunks)} å€‹å¡Šï¼Œé–‹å§‹ä¸¦ç™¼è½‰éŒ„...")
            
            # 2. åŠ è¼‰æ¨¡å‹ï¼ˆå…±äº«æ¨¡å‹å°è±¡ï¼Œä½†éœ€è¦æ³¨æ„ç·šç¨‹å®‰å…¨ï¼‰
            print("\n[1/3] æ­£åœ¨åŠ è¼‰æ¨¡å‹...")
            model = None
            try:
                model = whisperx.load_model(model_name, device, compute_type=compute_type)
            except Exception as load_error:
                error_str = str(load_error).lower()
                if ('ssl' in error_str or 'connection' in error_str or 'network' in error_str) and os.path.exists(model_cache_path):
                    print("âš  ç¶²çµ¡é€£æ¥å¤±æ•—ï¼Œå˜—è©¦é›¢ç·šæ¨¡å¼ä½¿ç”¨æœ¬åœ°ç·©å­˜...")
                    os.environ['HF_HUB_OFFLINE'] = '1'
                    try:
                        model = whisperx.load_model(model_name, device, compute_type=compute_type)
                        os.environ.pop('HF_HUB_OFFLINE', None)
                        print("âœ“ æˆåŠŸä½¿ç”¨æœ¬åœ°ç·©å­˜æ¨¡å‹")
                    except Exception as offline_error:
                        os.environ.pop('HF_HUB_OFFLINE', None)
                        raise Exception(f"ç„¡æ³•å¾æœ¬åœ°ç·©å­˜åŠ è¼‰æ¨¡å‹: {offline_error}. åŸå§‹éŒ¯èª¤: {load_error}")
                else:
                    raise
            
            # 3. å…ˆè½‰éŒ„ç¬¬ä¸€å€‹å¡Šä¾†æª¢æ¸¬èªè¨€ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
            detected_language = language
            if not detected_language and chunks:
                first_chunk_audio = whisperx.load_audio(chunks[0][0])
                first_result = model.transcribe(first_chunk_audio, batch_size=batch_size)
                detected_language = first_result.get('language', 'unknown')
                print(f"æª¢æ¸¬åˆ°çš„èªè¨€: {detected_language}")
            
            # 4. åŠ è¼‰å°é½Šæ¨¡å‹ï¼ˆå¦‚æœéœ€è¦å°é½Šï¼‰
            model_a = None
            align_metadata = None
            if detected_language:
                try:
                    model_a, align_metadata = whisperx.load_align_model(
                        language_code=detected_language,
                        device=device
                    )
                except Exception as e:
                    print(f"âš  ç„¡æ³•åŠ è¼‰å°é½Šæ¨¡å‹: {e}ï¼Œè·³éè©ç´šå°é½Š")
                    model_a = None
                    align_metadata = None
            
            # 5. ä¸¦ç™¼è½‰éŒ„æ‰€æœ‰å¡Š
            print(f"\n[2/3] æ­£åœ¨ä¸¦ç™¼è½‰éŒ„ {len(chunks)} å€‹éŸ³é »å¡Šï¼ˆæœ€å¤§ {max_workers} å€‹ç·šç¨‹ï¼‰...")
            results = []
            chunk_files_to_cleanup = []
            
            # ä½¿ç”¨ç·šç¨‹æ± ä¸¦ç™¼è½‰éŒ„
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_chunk = {}
                
                for idx, (chunk_file, chunk_start, chunk_end) in enumerate(chunks):
                    chunk_files_to_cleanup.append(chunk_file)
                    future = executor.submit(
                        transcribe_chunk,
                        chunk_file,
                        chunk_start,
                        model,
                        model_name,
                        device,
                        batch_size,
                        detected_language,
                        model_a,
                        align_metadata,
                        None  # ä¸ä½¿ç”¨åŸéŸ³é »æ•¸æ“š
                    )
                    future_to_chunk[future] = (idx, chunk_start)
                
                # æ”¶é›†çµæœ
                completed = 0
                for future in as_completed(future_to_chunk):
                    idx, chunk_start = future_to_chunk[future]
                    try:
                        chunk_result = future.result()
                        results.append(chunk_result)
                        completed += 1
                        
                        if progress_callback:
                            progress = 20 + int((completed / len(chunks)) * 50)
                            progress_callback(
                                progress,
                                100,
                                f"å·²è½‰éŒ„ {completed}/{len(chunks)} å€‹å¡Š ({completed/len(chunks)*100:.1f}%)"
                            )
                        print(f"âœ“ å¡Š {idx + 1}/{len(chunks)} è½‰éŒ„å®Œæˆ")
                    except Exception as e:
                        print(f"âš  å¡Š {idx + 1} è½‰éŒ„å¤±æ•—: {e}")
            
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            for chunk_file in chunk_files_to_cleanup:
                try:
                    if chunk_file != audio_file and os.path.exists(chunk_file):
                        os.remove(chunk_file)
                except Exception as e:
                    print(f"âš  æ¸…ç†è‡¨æ™‚æ–‡ä»¶å¤±æ•— {chunk_file}: {e}")
            
            # 6. åˆä½µçµæœ
            print("\n[3/3] æ­£åœ¨åˆä½µè½‰éŒ„çµæœ...")
            result = merge_transcription_results(results)
            result['language'] = detected_language or 'unknown'
            
            # æ¸…ç†æ¨¡å‹å…§å­˜
            if device == "cuda":
                gc.collect()
                import torch
                torch.cuda.empty_cache()
            
            if model:
                del model
            if model_a:
                del model_a
            
            print(f"åˆä½µå¾Œçš„æ®µè½æ•¸: {len(result['segments'])}")
            
            # 7. èªªè©±äººåˆ†é›¢ï¼ˆå¦‚æœå•Ÿç”¨ï¼Œéœ€è¦åœ¨å®Œæ•´éŸ³é »ä¸Šé€²è¡Œï¼‰
            if diarize:
                if hf_token is None:
                    print("âš  è­¦å‘Š: éœ€è¦HuggingFace tokenæ‰èƒ½é€²è¡Œèªªè©±äººåˆ†é›¢")
                    diarize = False
                else:
                    print("\n[4/4] æ­£åœ¨é€²è¡Œèªªè©±äººåˆ†é›¢ï¼ˆéœ€è¦åœ¨å®Œæ•´éŸ³é »ä¸ŠåŸ·è¡Œï¼‰...")
                    from whisperx.diarize import DiarizationPipeline
                    
                    audio = whisperx.load_audio(audio_file)
                    diarize_model = DiarizationPipeline(
                        use_auth_token=hf_token,
                        device=device
                    )
                    diarize_segments = diarize_model(audio)
                    result = whisperx.assign_word_speakers(diarize_segments, result)
                    
                    print(f"æª¢æ¸¬åˆ°çš„èªªè©±äººæ•¸é‡: {len(set([seg.get('speaker', 'UNKNOWN') for seg in diarize_segments]))}")
            
        else:
            # ä½¿ç”¨åŸæœ‰æ–¹æ³•ï¼ˆä¸åˆ†å¡Šï¼‰
            print("\n[ä½¿ç”¨å‚³çµ±æ–¹æ³•] éŸ³é »è¼ƒçŸ­æˆ–æœªå•Ÿç”¨åˆ†å¡Šï¼Œä½¿ç”¨å®Œæ•´è½‰éŒ„...")
            
            # 1. åŠ è¼‰æ¨¡å‹ä¸¦è½‰éŒ„
            print("\n[1/3] æ­£åœ¨åŠ è¼‰æ¨¡å‹ä¸¦è½‰éŒ„...")
            
        model = None
        try:
            model = whisperx.load_model(model_name, device, compute_type=compute_type)
        except Exception as load_error:
            error_str = str(load_error).lower()
            if ('ssl' in error_str or 'connection' in error_str or 'network' in error_str) and os.path.exists(model_cache_path):
                print("âš  ç¶²çµ¡é€£æ¥å¤±æ•—ï¼Œå˜—è©¦é›¢ç·šæ¨¡å¼ä½¿ç”¨æœ¬åœ°ç·©å­˜...")
                os.environ['HF_HUB_OFFLINE'] = '1'
                try:
                    model = whisperx.load_model(model_name, device, compute_type=compute_type)
                    os.environ.pop('HF_HUB_OFFLINE', None)
                    print("âœ“ æˆåŠŸä½¿ç”¨æœ¬åœ°ç·©å­˜æ¨¡å‹")
                except Exception as offline_error:
                    os.environ.pop('HF_HUB_OFFLINE', None)
                    raise Exception(f"ç„¡æ³•å¾æœ¬åœ°ç·©å­˜åŠ è¼‰æ¨¡å‹: {offline_error}. åŸå§‹éŒ¯èª¤: {load_error}")
            else:
                raise
        
        audio = whisperx.load_audio(audio_file)
        result = model.transcribe(audio, batch_size=batch_size, language=language)
        
        print(f"æª¢æ¸¬åˆ°çš„èªè¨€: {result['language']}")
        print(f"è½‰éŒ„æ®µè½æ•¸: {len(result['segments'])}")
        
        # æ¸…ç†æ¨¡å‹å…§å­˜ï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
        if device == "cuda":
            gc.collect()
            import torch
            torch.cuda.empty_cache()
            del model
        
        # 2. å°é½Šæ™‚é–“æˆ³ï¼ˆè©ç´šåˆ¥ï¼‰
        print("\n[2/3] æ­£åœ¨å°é½Šè©ç´šæ™‚é–“æˆ³...")
        model_a = None
        try:
            model_a, metadata = whisperx.load_align_model(
                language_code=result["language"], 
                device=device
            )
            
            # è¿‡æ»¤æ‰ç©ºæ–‡æœ¬æˆ–æ— æ•ˆçš„æ®µè½
            valid_segments = [
                seg for seg in result["segments"]
                if seg.get('text', '').strip() and len(seg.get('text', '').strip()) > 0
            ]
            
            if valid_segments:
                aligned_result = whisperx.align(
                    valid_segments, 
                    model_a, 
                    metadata, 
                    audio, 
                    device,
                    return_char_alignments=False
                )
                
                # åˆå¹¶å¯¹é½åçš„æ®µè½å’Œæœªå¯¹é½çš„æ®µè½
                aligned_segments = aligned_result.get('segments', [])
                aligned_texts = {seg.get('text', '').strip() for seg in aligned_segments if seg.get('text', '').strip()}
                
                # ä¿ç•™æ— æ³•å¯¹é½çš„æ®µè½ï¼ˆæ²¡æœ‰æ–‡æœ¬åŒ¹é…çš„ï¼‰
                for seg in result["segments"]:
                    if seg.get('text', '').strip() not in aligned_texts:
                        aligned_segments.append(seg)
                
                result['segments'] = aligned_segments
                print(f"âœ“ å°é½Šå®Œæˆ: {len([s for s in aligned_segments if 'words' in s])} å€‹æ®µè½æœ‰è©ç´šæ™‚é–“æˆ³")
            else:
                print("âš  æ²’æœ‰æœ‰æ•ˆæ®µè½éœ€è¦å°é½Š")
                
        except Exception as align_error:
            print(f"âš  å°é½Šéç¨‹å‡ºéŒ¯ï¼Œä½¿ç”¨åŸå§‹è½‰éŒ„çµæœ: {align_error}")
            # ä¿æŒåŸå§‹ç»“æœï¼Œä¸å½±å“åç»­å¤„ç†
            pass
        
        print(f"å°é½Šå¾Œçš„æ®µè½æ•¸: {len(result.get('segments', []))}")
        
        # æ¸…ç†å°é½Šæ¨¡å‹å…§å­˜
        if model_a:
            if device == "cuda":
                gc.collect()
                import torch
                torch.cuda.empty_cache()
            
            try:
                del model_a
            except:
                pass
        
        # 3. èªªè©±äººåˆ†é›¢ï¼ˆå¯é¸ï¼‰
        if diarize:
            if hf_token is None:
                print("âš  è­¦å‘Š: éœ€è¦HuggingFace tokenæ‰èƒ½é€²è¡Œèªªè©±äººåˆ†é›¢")
                print("   è¨­ç½®ç’°å¢ƒè®Šé‡ HF_TOKEN æˆ–å‚³å…¥ hf_token åƒæ•¸")
                diarize = False
        
        if diarize:
            print("\n[3/3] æ­£åœ¨é€²è¡Œèªªè©±äººåˆ†é›¢...")
            from whisperx.diarize import DiarizationPipeline
            
            diarize_model = DiarizationPipeline(
                use_auth_token=hf_token,
                device=device
            )
            diarize_segments = diarize_model(audio)
            result = whisperx.assign_word_speakers(diarize_segments, result)
            
            print(f"æª¢æ¸¬åˆ°çš„èªªè©±äººæ•¸é‡: {len(set([seg.get('speaker', 'UNKNOWN') for seg in diarize_segments]))}")
        else:
            print("\n[3/3] è·³éèªªè©±äººåˆ†é›¢")
        
        # ä¿å­˜çµæœ
        base_name = Path(audio_file).stem
        output_path = save_transcription_result(
            result, 
            output_dir, 
            base_name,
            highlight_words=highlight_words
        )
        
        result['output_file'] = output_path
        result['duration'] = audio_duration
        result['estimated_time'] = estimated_time
        
        print(f"\nâœ“ è½‰éŒ„å®Œæˆï¼")
        print(f"è¼¸å‡ºæ–‡ä»¶: {output_path}")
        
        return result
        
    except Exception as e:
        print(f"âŒ è½‰éŒ„éŒ¯èª¤: {e}")
        raise


def save_transcription_result(
    result: Dict[str, Any],
    output_dir: str,
    base_name: str,
    highlight_words: bool = False
) -> str:
    """
    ä¿å­˜è½‰éŒ„çµæœç‚ºå¤šç¨®æ ¼å¼
    
    åƒæ•¸:
        result: WhisperXè½‰éŒ„çµæœ
        output_dir: è¼¸å‡ºç›®éŒ„
        base_name: åŸºç¤æ–‡ä»¶å
        highlight_words: æ˜¯å¦åœ¨SRTä¸­é«˜äº®è©ç´šæ™‚é–“æˆ³
    
    è¿”å›:
        str: ä¸»è¦è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
    """
    output_dir = Path(output_dir)
    
    # 1. ä¿å­˜ç‚ºæ–‡æœ¬æ–‡ä»¶
    txt_file = output_dir / f"{base_name}_transcript.txt"
    with open(txt_file, 'w', encoding='utf-8') as f:
        for segment in result['segments']:
            text = segment.get('text', '').strip()
            if text:
                f.write(text + '\n')
    
    # 2. ä¿å­˜ç‚ºSRTå­—å¹•æ–‡ä»¶
    srt_file = output_dir / f"{base_name}_transcript.srt"
    with open(srt_file, 'w', encoding='utf-8') as f:
        for idx, segment in enumerate(result['segments'], 1):
            start = format_timestamp(segment['start'])
            end = format_timestamp(segment['end'])
            
            # ç²å–èªªè©±äººæ¨™ç±¤ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            speaker = segment.get('speaker', '')
            text = segment.get('text', '').strip()
            
            f.write(f"{idx}\n")
            f.write(f"{start} --> {end}\n")
            if speaker:
                f.write(f"[{speaker}] {text}\n")
            else:
                f.write(f"{text}\n")
            f.write("\n")
            
            # å¦‚æœæœ‰è©ç´šæ™‚é–“æˆ³ä¸”å•Ÿç”¨é«˜äº®
            if highlight_words and 'words' in segment:
                for word_info in segment['words']:
                    word = word_info.get('word', '')
                    word_start = format_timestamp(word_info.get('start', 0))
                    word_end = format_timestamp(word_info.get('end', 0))
                    f.write(f"{idx}.{word_info.get('id', 0)}\n")
                    f.write(f"{word_start} --> {word_end}\n")
                    f.write(f"<font color=\"#ffff00\">{word}</font>\n")
                    f.write("\n")
    
    # 3. ä¿å­˜ç‚ºJSONæ–‡ä»¶ï¼ˆå®Œæ•´ä¿¡æ¯ï¼‰
    import json
    json_file = output_dir / f"{base_name}_transcript.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    return str(txt_file)


def format_timestamp(seconds: float) -> str:
    """å°‡ç§’æ•¸æ ¼å¼åŒ–ç‚ºSRTæ™‚é–“æˆ³æ ¼å¼ (HH:MM:SS,mmm)"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"


def transcribe_audio_simple(audio_file: str, model_name: str = "base") -> Dict[str, Any]:
    """
    ç°¡å–®ç‰ˆæœ¬ï¼šä½¿ç”¨é»˜èªè¨­ç½®è½‰éŒ„éŸ³é »
    
    åƒæ•¸:
        audio_file (str): éŸ³é »æ–‡ä»¶è·¯å¾‘
        model_name (str): Whisperæ¨¡å‹åç¨±
    
    è¿”å›:
        dict: è½‰éŒ„çµæœ
    """
    return transcribe_audio(audio_file, model_name=model_name)


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python transcribe_audio.py <éŸ³é »æ–‡ä»¶> [æ¨¡å‹åç¨±]")
        print("ç¤ºä¾‹: python transcribe_audio.py audio.mp3 base")
        print("æ¨¡å‹é¸é …: tiny, base, small, medium, large-v2, large-v3")
        sys.exit(1)
    
    audio_file = sys.argv[1]
    model_name = sys.argv[2] if len(sys.argv) > 2 else "base"
    
    # å¾ç’°å¢ƒè®Šé‡ç²å–HuggingFace token
    hf_token = os.getenv("HF_TOKEN")
    
    print("=" * 60)
    print("WhisperX éŸ³é »è½‰æ–‡æœ¬")
    print("=" * 60)
    
    try:
        result = transcribe_audio(
            audio_file,
            model_name=model_name,
            diarize=False,  # å¦‚æœéœ€è¦èªªè©±äººåˆ†é›¢ï¼Œè¨­ç½®ç‚ºTrueä¸¦æä¾›HF_TOKEN
            hf_token=hf_token
        )
        
        print(f"\nè½‰éŒ„æ‘˜è¦:")
        print(f"- èªè¨€: {result.get('language', 'Unknown')}")
        print(f"- æ®µè½æ•¸: {len(result.get('segments', []))}")
        
        # é¡¯ç¤ºå‰å¹¾å€‹æ®µè½
        print(f"\nå‰3å€‹æ®µè½é è¦½:")
        for i, segment in enumerate(result.get('segments', [])[:3], 1):
            print(f"{i}. [{format_timestamp(segment['start'])} - {format_timestamp(segment['end'])}]")
            print(f"   {segment.get('text', '').strip()}")
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

